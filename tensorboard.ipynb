{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "        # MNIST中的图像默认为uint8（0-255的数字）。以下代码将其归一化到0-1之间的浮点数，并在最后增加一维作为颜色通道\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        # 从数据集中随机取出batch_size个元素并返回\n",
    "        index = np.random.randint(0, np.shape(self.train_data)[0], batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.flatten(inputs)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        outputs = tf.nn.softmax(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 1000\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "model = MLP()\n",
    "data_loader = MNISTLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例化记录器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.471189\n",
      "batch 1: loss 2.288918\n",
      "batch 2: loss 2.175646\n",
      "batch 3: loss 2.183202\n",
      "batch 4: loss 2.141355\n",
      "batch 5: loss 2.059717\n",
      "batch 6: loss 2.042632\n",
      "batch 7: loss 1.918728\n",
      "batch 8: loss 1.915061\n",
      "batch 9: loss 1.838109\n",
      "batch 10: loss 1.880540\n",
      "batch 11: loss 1.760955\n",
      "batch 12: loss 1.643969\n",
      "batch 13: loss 1.547007\n",
      "batch 14: loss 1.585948\n",
      "batch 15: loss 1.623557\n",
      "batch 16: loss 1.348319\n",
      "batch 17: loss 1.465632\n",
      "batch 18: loss 1.323162\n",
      "batch 19: loss 1.155519\n",
      "batch 20: loss 1.384053\n",
      "batch 21: loss 1.334720\n",
      "batch 22: loss 1.277134\n",
      "batch 23: loss 1.279007\n",
      "batch 24: loss 1.029177\n",
      "batch 25: loss 1.069543\n",
      "batch 26: loss 0.982290\n",
      "batch 27: loss 1.024232\n",
      "batch 28: loss 0.927851\n",
      "batch 29: loss 0.984752\n",
      "batch 30: loss 0.989552\n",
      "batch 31: loss 0.798216\n",
      "batch 32: loss 0.966500\n",
      "batch 33: loss 1.008278\n",
      "batch 34: loss 0.659489\n",
      "batch 35: loss 0.931288\n",
      "batch 36: loss 0.779225\n",
      "batch 37: loss 0.754948\n",
      "batch 38: loss 0.774880\n",
      "batch 39: loss 0.606378\n",
      "batch 40: loss 0.837299\n",
      "batch 41: loss 0.814988\n",
      "batch 42: loss 0.711381\n",
      "batch 43: loss 0.833238\n",
      "batch 44: loss 0.739039\n",
      "batch 45: loss 0.835907\n",
      "batch 46: loss 0.478035\n",
      "batch 47: loss 0.710327\n",
      "batch 48: loss 0.796870\n",
      "batch 49: loss 0.597567\n",
      "batch 50: loss 0.570501\n",
      "batch 51: loss 0.483808\n",
      "batch 52: loss 0.541136\n",
      "batch 53: loss 0.569400\n",
      "batch 54: loss 0.847989\n",
      "batch 55: loss 0.864478\n",
      "batch 56: loss 0.579503\n",
      "batch 57: loss 0.555940\n",
      "batch 58: loss 0.656619\n",
      "batch 59: loss 0.581946\n",
      "batch 60: loss 0.479879\n",
      "batch 61: loss 0.584261\n",
      "batch 62: loss 0.492726\n",
      "batch 63: loss 0.407512\n",
      "batch 64: loss 0.548728\n",
      "batch 65: loss 0.590907\n",
      "batch 66: loss 0.591744\n",
      "batch 67: loss 0.561585\n",
      "batch 68: loss 0.506783\n",
      "batch 69: loss 0.638210\n",
      "batch 70: loss 0.665195\n",
      "batch 71: loss 0.489231\n",
      "batch 72: loss 0.550462\n",
      "batch 73: loss 0.546738\n",
      "batch 74: loss 0.379423\n",
      "batch 75: loss 0.623450\n",
      "batch 76: loss 0.340325\n",
      "batch 77: loss 0.303235\n",
      "batch 78: loss 0.526558\n",
      "batch 79: loss 0.572798\n",
      "batch 80: loss 0.597692\n",
      "batch 81: loss 0.736981\n",
      "batch 82: loss 0.466595\n",
      "batch 83: loss 0.411502\n",
      "batch 84: loss 0.427812\n",
      "batch 85: loss 0.373960\n",
      "batch 86: loss 0.395555\n",
      "batch 87: loss 0.495060\n",
      "batch 88: loss 0.537352\n",
      "batch 89: loss 0.603050\n",
      "batch 90: loss 0.325703\n",
      "batch 91: loss 0.491985\n",
      "batch 92: loss 0.223670\n",
      "batch 93: loss 0.311052\n",
      "batch 94: loss 0.420038\n",
      "batch 95: loss 0.555636\n",
      "batch 96: loss 0.485692\n",
      "batch 97: loss 0.550534\n",
      "batch 98: loss 0.439633\n",
      "batch 99: loss 0.501729\n",
      "batch 100: loss 0.510646\n",
      "batch 101: loss 0.441883\n",
      "batch 102: loss 0.286566\n",
      "batch 103: loss 0.450813\n",
      "batch 104: loss 0.385307\n",
      "batch 105: loss 0.324148\n",
      "batch 106: loss 0.553979\n",
      "batch 107: loss 0.581981\n",
      "batch 108: loss 0.531630\n",
      "batch 109: loss 0.452661\n",
      "batch 110: loss 0.413685\n",
      "batch 111: loss 0.438399\n",
      "batch 112: loss 0.302751\n",
      "batch 113: loss 0.631584\n",
      "batch 114: loss 0.684406\n",
      "batch 115: loss 0.352790\n",
      "batch 116: loss 0.405393\n",
      "batch 117: loss 0.368335\n",
      "batch 118: loss 0.473745\n",
      "batch 119: loss 0.277566\n",
      "batch 120: loss 0.485291\n",
      "batch 121: loss 0.319842\n",
      "batch 122: loss 0.301614\n",
      "batch 123: loss 0.307219\n",
      "batch 124: loss 0.325560\n",
      "batch 125: loss 0.379983\n",
      "batch 126: loss 0.414758\n",
      "batch 127: loss 0.426794\n",
      "batch 128: loss 0.596636\n",
      "batch 129: loss 0.285173\n",
      "batch 130: loss 0.349514\n",
      "batch 131: loss 0.393549\n",
      "batch 132: loss 0.372083\n",
      "batch 133: loss 0.370654\n",
      "batch 134: loss 0.302154\n",
      "batch 135: loss 0.418682\n",
      "batch 136: loss 0.166696\n",
      "batch 137: loss 0.314145\n",
      "batch 138: loss 0.336207\n",
      "batch 139: loss 0.328777\n",
      "batch 140: loss 0.596163\n",
      "batch 141: loss 0.429015\n",
      "batch 142: loss 0.451933\n",
      "batch 143: loss 0.438985\n",
      "batch 144: loss 0.369305\n",
      "batch 145: loss 0.391667\n",
      "batch 146: loss 0.459696\n",
      "batch 147: loss 0.296144\n",
      "batch 148: loss 0.411984\n",
      "batch 149: loss 0.401352\n",
      "batch 150: loss 0.462275\n",
      "batch 151: loss 0.428335\n",
      "batch 152: loss 0.426328\n",
      "batch 153: loss 0.505197\n",
      "batch 154: loss 0.165324\n",
      "batch 155: loss 0.407268\n",
      "batch 156: loss 0.379905\n",
      "batch 157: loss 0.350050\n",
      "batch 158: loss 0.292992\n",
      "batch 159: loss 0.379442\n",
      "batch 160: loss 0.470831\n",
      "batch 161: loss 0.387141\n",
      "batch 162: loss 0.350067\n",
      "batch 163: loss 0.225381\n",
      "batch 164: loss 0.385423\n",
      "batch 165: loss 0.326239\n",
      "batch 166: loss 0.418427\n",
      "batch 167: loss 0.309913\n",
      "batch 168: loss 0.170085\n",
      "batch 169: loss 0.437855\n",
      "batch 170: loss 0.336139\n",
      "batch 171: loss 0.269447\n",
      "batch 172: loss 0.238274\n",
      "batch 173: loss 0.462584\n",
      "batch 174: loss 0.348525\n",
      "batch 175: loss 0.353734\n",
      "batch 176: loss 0.343549\n",
      "batch 177: loss 0.245149\n",
      "batch 178: loss 0.288356\n",
      "batch 179: loss 0.496705\n",
      "batch 180: loss 0.400206\n",
      "batch 181: loss 0.304647\n",
      "batch 182: loss 0.404436\n",
      "batch 183: loss 0.553743\n",
      "batch 184: loss 0.183118\n",
      "batch 185: loss 0.372041\n",
      "batch 186: loss 0.328834\n",
      "batch 187: loss 0.319153\n",
      "batch 188: loss 0.338691\n",
      "batch 189: loss 0.503887\n",
      "batch 190: loss 0.441814\n",
      "batch 191: loss 0.237736\n",
      "batch 192: loss 0.218535\n",
      "batch 193: loss 0.344877\n",
      "batch 194: loss 0.607539\n",
      "batch 195: loss 0.392481\n",
      "batch 196: loss 0.173576\n",
      "batch 197: loss 0.501258\n",
      "batch 198: loss 0.451918\n",
      "batch 199: loss 0.253168\n",
      "batch 200: loss 0.315387\n",
      "batch 201: loss 0.552519\n",
      "batch 202: loss 0.417225\n",
      "batch 203: loss 0.363587\n",
      "batch 204: loss 0.207780\n",
      "batch 205: loss 0.422396\n",
      "batch 206: loss 0.313404\n",
      "batch 207: loss 0.296326\n",
      "batch 208: loss 0.413752\n",
      "batch 209: loss 0.592535\n",
      "batch 210: loss 0.480053\n",
      "batch 211: loss 0.381466\n",
      "batch 212: loss 0.468531\n",
      "batch 213: loss 0.292073\n",
      "batch 214: loss 0.301323\n",
      "batch 215: loss 0.284860\n",
      "batch 216: loss 0.318364\n",
      "batch 217: loss 0.521633\n",
      "batch 218: loss 0.379006\n",
      "batch 219: loss 0.509770\n",
      "batch 220: loss 0.339726\n",
      "batch 221: loss 0.369695\n",
      "batch 222: loss 0.369796\n",
      "batch 223: loss 0.369429\n",
      "batch 224: loss 0.163645\n",
      "batch 225: loss 0.373498\n",
      "batch 226: loss 0.567140\n",
      "batch 227: loss 0.270443\n",
      "batch 228: loss 0.328965\n",
      "batch 229: loss 0.325861\n",
      "batch 230: loss 0.351832\n",
      "batch 231: loss 0.359001\n",
      "batch 232: loss 0.427360\n",
      "batch 233: loss 0.168563\n",
      "batch 234: loss 0.309996\n",
      "batch 235: loss 0.193803\n",
      "batch 236: loss 0.282201\n",
      "batch 237: loss 0.621157\n",
      "batch 238: loss 0.307619\n",
      "batch 239: loss 0.162532\n",
      "batch 240: loss 0.276070\n",
      "batch 241: loss 0.169421\n",
      "batch 242: loss 0.227491\n",
      "batch 243: loss 0.344071\n",
      "batch 244: loss 0.363001\n",
      "batch 245: loss 0.409851\n",
      "batch 246: loss 0.686515\n",
      "batch 247: loss 0.552406\n",
      "batch 248: loss 0.239609\n",
      "batch 249: loss 0.299951\n",
      "batch 250: loss 0.269525\n",
      "batch 251: loss 0.387294\n",
      "batch 252: loss 0.205048\n",
      "batch 253: loss 0.242062\n",
      "batch 254: loss 0.162392\n",
      "batch 255: loss 0.324967\n",
      "batch 256: loss 0.194565\n",
      "batch 257: loss 0.268285\n",
      "batch 258: loss 0.211364\n",
      "batch 259: loss 0.239320\n",
      "batch 260: loss 0.444022\n",
      "batch 261: loss 0.229690\n",
      "batch 262: loss 0.401637\n",
      "batch 263: loss 0.177011\n",
      "batch 264: loss 0.252297\n",
      "batch 265: loss 0.225083\n",
      "batch 266: loss 0.275576\n",
      "batch 267: loss 0.169959\n",
      "batch 268: loss 0.241879\n",
      "batch 269: loss 0.460372\n",
      "batch 270: loss 0.264532\n",
      "batch 271: loss 0.362603\n",
      "batch 272: loss 0.194683\n",
      "batch 273: loss 0.213294\n",
      "batch 274: loss 0.508246\n",
      "batch 275: loss 0.162305\n",
      "batch 276: loss 0.202101\n",
      "batch 277: loss 0.287045\n",
      "batch 278: loss 0.310796\n",
      "batch 279: loss 0.315336\n",
      "batch 280: loss 0.296382\n",
      "batch 281: loss 0.465169\n",
      "batch 282: loss 0.306463\n",
      "batch 283: loss 0.357595\n",
      "batch 284: loss 0.371937\n",
      "batch 285: loss 0.319998\n",
      "batch 286: loss 0.202234\n",
      "batch 287: loss 0.236845\n",
      "batch 288: loss 0.247219\n",
      "batch 289: loss 0.304977\n",
      "batch 290: loss 0.260105\n",
      "batch 291: loss 0.349466\n",
      "batch 292: loss 0.253974\n",
      "batch 293: loss 0.230976\n",
      "batch 294: loss 0.343488\n",
      "batch 295: loss 0.359068\n",
      "batch 296: loss 0.218992\n",
      "batch 297: loss 0.203092\n",
      "batch 298: loss 0.368598\n",
      "batch 299: loss 0.260346\n",
      "batch 300: loss 0.383482\n",
      "batch 301: loss 0.448084\n",
      "batch 302: loss 0.209812\n",
      "batch 303: loss 0.213300\n",
      "batch 304: loss 0.163919\n",
      "batch 305: loss 0.254149\n",
      "batch 306: loss 0.254416\n",
      "batch 307: loss 0.248198\n",
      "batch 308: loss 0.288298\n",
      "batch 309: loss 0.279116\n",
      "batch 310: loss 0.357581\n",
      "batch 311: loss 0.198420\n",
      "batch 312: loss 0.255953\n",
      "batch 313: loss 0.311936\n",
      "batch 314: loss 0.252634\n",
      "batch 315: loss 0.524761\n",
      "batch 316: loss 0.460416\n",
      "batch 317: loss 0.172370\n",
      "batch 318: loss 0.274194\n",
      "batch 319: loss 0.379519\n",
      "batch 320: loss 0.192313\n",
      "batch 321: loss 0.270397\n",
      "batch 322: loss 0.288514\n",
      "batch 323: loss 0.429616\n",
      "batch 324: loss 0.252285\n",
      "batch 325: loss 0.158225\n",
      "batch 326: loss 0.263066\n",
      "batch 327: loss 0.199169\n",
      "batch 328: loss 0.156040\n",
      "batch 329: loss 0.275704\n",
      "batch 330: loss 0.327398\n",
      "batch 331: loss 0.139630\n",
      "batch 332: loss 0.196670\n",
      "batch 333: loss 0.304558\n",
      "batch 334: loss 0.367864\n",
      "batch 335: loss 0.115154\n",
      "batch 336: loss 0.238349\n",
      "batch 337: loss 0.401510\n",
      "batch 338: loss 0.139464\n",
      "batch 339: loss 0.314649\n",
      "batch 340: loss 0.292698\n",
      "batch 341: loss 0.413164\n",
      "batch 342: loss 0.235206\n",
      "batch 343: loss 0.273676\n",
      "batch 344: loss 0.220114\n",
      "batch 345: loss 0.299222\n",
      "batch 346: loss 0.205094\n",
      "batch 347: loss 0.228811\n",
      "batch 348: loss 0.399483\n",
      "batch 349: loss 0.209228\n",
      "batch 350: loss 0.216863\n",
      "batch 351: loss 0.184729\n",
      "batch 352: loss 0.206416\n",
      "batch 353: loss 0.148709\n",
      "batch 354: loss 0.554820\n",
      "batch 355: loss 0.563317\n",
      "batch 356: loss 0.189114\n",
      "batch 357: loss 0.189436\n",
      "batch 358: loss 0.321553\n",
      "batch 359: loss 0.410636\n",
      "batch 360: loss 0.306679\n",
      "batch 361: loss 0.385021\n",
      "batch 362: loss 0.309076\n",
      "batch 363: loss 0.315006\n",
      "batch 364: loss 0.276708\n",
      "batch 365: loss 0.292614\n",
      "batch 366: loss 0.245317\n",
      "batch 367: loss 0.311858\n",
      "batch 368: loss 0.200603\n",
      "batch 369: loss 0.245498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 370: loss 0.468580\n",
      "batch 371: loss 0.391464\n",
      "batch 372: loss 0.175849\n",
      "batch 373: loss 0.370556\n",
      "batch 374: loss 0.182336\n",
      "batch 375: loss 0.159059\n",
      "batch 376: loss 0.132172\n",
      "batch 377: loss 0.231616\n",
      "batch 378: loss 0.449671\n",
      "batch 379: loss 0.253689\n",
      "batch 380: loss 0.184474\n",
      "batch 381: loss 0.223852\n",
      "batch 382: loss 0.350857\n",
      "batch 383: loss 0.245531\n",
      "batch 384: loss 0.202812\n",
      "batch 385: loss 0.271666\n",
      "batch 386: loss 0.302183\n",
      "batch 387: loss 0.118713\n",
      "batch 388: loss 0.303552\n",
      "batch 389: loss 0.306952\n",
      "batch 390: loss 0.275214\n",
      "batch 391: loss 0.246994\n",
      "batch 392: loss 0.231921\n",
      "batch 393: loss 0.133198\n",
      "batch 394: loss 0.331932\n",
      "batch 395: loss 0.427485\n",
      "batch 396: loss 0.254808\n",
      "batch 397: loss 0.303857\n",
      "batch 398: loss 0.267579\n",
      "batch 399: loss 0.404240\n",
      "batch 400: loss 0.186484\n",
      "batch 401: loss 0.149177\n",
      "batch 402: loss 0.407237\n",
      "batch 403: loss 0.501164\n",
      "batch 404: loss 0.331019\n",
      "batch 405: loss 0.292393\n",
      "batch 406: loss 0.239546\n",
      "batch 407: loss 0.158568\n",
      "batch 408: loss 0.200633\n",
      "batch 409: loss 0.209336\n",
      "batch 410: loss 0.257748\n",
      "batch 411: loss 0.268806\n",
      "batch 412: loss 0.249825\n",
      "batch 413: loss 0.275909\n",
      "batch 414: loss 0.461940\n",
      "batch 415: loss 0.201081\n",
      "batch 416: loss 0.221302\n",
      "batch 417: loss 0.205357\n",
      "batch 418: loss 0.340020\n",
      "batch 419: loss 0.447967\n",
      "batch 420: loss 0.159690\n",
      "batch 421: loss 0.317682\n",
      "batch 422: loss 0.181527\n",
      "batch 423: loss 0.285803\n",
      "batch 424: loss 0.192000\n",
      "batch 425: loss 0.238173\n",
      "batch 426: loss 0.426743\n",
      "batch 427: loss 0.252472\n",
      "batch 428: loss 0.264453\n",
      "batch 429: loss 0.450081\n",
      "batch 430: loss 0.292092\n",
      "batch 431: loss 0.366469\n",
      "batch 432: loss 0.253137\n",
      "batch 433: loss 0.429095\n",
      "batch 434: loss 0.290151\n",
      "batch 435: loss 0.168780\n",
      "batch 436: loss 0.197770\n",
      "batch 437: loss 0.254900\n",
      "batch 438: loss 0.282762\n",
      "batch 439: loss 0.209249\n",
      "batch 440: loss 0.354796\n",
      "batch 441: loss 0.285631\n",
      "batch 442: loss 0.170406\n",
      "batch 443: loss 0.258527\n",
      "batch 444: loss 0.226034\n",
      "batch 445: loss 0.172625\n",
      "batch 446: loss 0.282813\n",
      "batch 447: loss 0.115861\n",
      "batch 448: loss 0.216811\n",
      "batch 449: loss 0.121106\n",
      "batch 450: loss 0.156061\n",
      "batch 451: loss 0.160941\n",
      "batch 452: loss 0.182091\n",
      "batch 453: loss 0.195447\n",
      "batch 454: loss 0.239829\n",
      "batch 455: loss 0.136242\n",
      "batch 456: loss 0.221207\n",
      "batch 457: loss 0.279637\n",
      "batch 458: loss 0.217765\n",
      "batch 459: loss 0.340161\n",
      "batch 460: loss 0.117739\n",
      "batch 461: loss 0.213021\n",
      "batch 462: loss 0.138659\n",
      "batch 463: loss 0.280922\n",
      "batch 464: loss 0.351604\n",
      "batch 465: loss 0.211398\n",
      "batch 466: loss 0.424151\n",
      "batch 467: loss 0.258559\n",
      "batch 468: loss 0.232080\n",
      "batch 469: loss 0.376361\n",
      "batch 470: loss 0.278463\n",
      "batch 471: loss 0.324484\n",
      "batch 472: loss 0.232168\n",
      "batch 473: loss 0.225831\n",
      "batch 474: loss 0.448207\n",
      "batch 475: loss 0.234975\n",
      "batch 476: loss 0.146007\n",
      "batch 477: loss 0.213135\n",
      "batch 478: loss 0.200664\n",
      "batch 479: loss 0.168038\n",
      "batch 480: loss 0.290179\n",
      "batch 481: loss 0.120000\n",
      "batch 482: loss 0.130039\n",
      "batch 483: loss 0.209261\n",
      "batch 484: loss 0.156462\n",
      "batch 485: loss 0.405893\n",
      "batch 486: loss 0.269713\n",
      "batch 487: loss 0.241229\n",
      "batch 488: loss 0.154166\n",
      "batch 489: loss 0.223709\n",
      "batch 490: loss 0.521633\n",
      "batch 491: loss 0.223698\n",
      "batch 492: loss 0.246791\n",
      "batch 493: loss 0.303658\n",
      "batch 494: loss 0.145068\n",
      "batch 495: loss 0.219999\n",
      "batch 496: loss 0.455873\n",
      "batch 497: loss 0.307874\n",
      "batch 498: loss 0.301155\n",
      "batch 499: loss 0.429111\n",
      "batch 500: loss 0.179074\n",
      "batch 501: loss 0.291263\n",
      "batch 502: loss 0.306230\n",
      "batch 503: loss 0.324356\n",
      "batch 504: loss 0.090583\n",
      "batch 505: loss 0.317195\n",
      "batch 506: loss 0.241024\n",
      "batch 507: loss 0.151959\n",
      "batch 508: loss 0.215310\n",
      "batch 509: loss 0.441004\n",
      "batch 510: loss 0.323151\n",
      "batch 511: loss 0.111020\n",
      "batch 512: loss 0.193732\n",
      "batch 513: loss 0.248341\n",
      "batch 514: loss 0.138129\n",
      "batch 515: loss 0.209357\n",
      "batch 516: loss 0.213168\n",
      "batch 517: loss 0.311817\n",
      "batch 518: loss 0.223393\n",
      "batch 519: loss 0.246313\n",
      "batch 520: loss 0.215588\n",
      "batch 521: loss 0.214042\n",
      "batch 522: loss 0.217610\n",
      "batch 523: loss 0.168963\n",
      "batch 524: loss 0.261333\n",
      "batch 525: loss 0.262879\n",
      "batch 526: loss 0.207326\n",
      "batch 527: loss 0.156987\n",
      "batch 528: loss 0.252711\n",
      "batch 529: loss 0.357136\n",
      "batch 530: loss 0.123339\n",
      "batch 531: loss 0.265238\n",
      "batch 532: loss 0.138879\n",
      "batch 533: loss 0.259797\n",
      "batch 534: loss 0.166858\n",
      "batch 535: loss 0.152888\n",
      "batch 536: loss 0.120836\n",
      "batch 537: loss 0.257923\n",
      "batch 538: loss 0.143943\n",
      "batch 539: loss 0.296725\n",
      "batch 540: loss 0.330811\n",
      "batch 541: loss 0.362257\n",
      "batch 542: loss 0.359186\n",
      "batch 543: loss 0.236073\n",
      "batch 544: loss 0.169185\n",
      "batch 545: loss 0.264766\n",
      "batch 546: loss 0.219776\n",
      "batch 547: loss 0.179469\n",
      "batch 548: loss 0.224731\n",
      "batch 549: loss 0.215219\n",
      "batch 550: loss 0.129645\n",
      "batch 551: loss 0.072421\n",
      "batch 552: loss 0.051528\n",
      "batch 553: loss 0.286696\n",
      "batch 554: loss 0.181398\n",
      "batch 555: loss 0.192820\n",
      "batch 556: loss 0.149693\n",
      "batch 557: loss 0.159471\n",
      "batch 558: loss 0.169524\n",
      "batch 559: loss 0.356223\n",
      "batch 560: loss 0.222616\n",
      "batch 561: loss 0.201326\n",
      "batch 562: loss 0.489776\n",
      "batch 563: loss 0.181755\n",
      "batch 564: loss 0.150080\n",
      "batch 565: loss 0.106651\n",
      "batch 566: loss 0.272718\n",
      "batch 567: loss 0.234373\n",
      "batch 568: loss 0.164307\n",
      "batch 569: loss 0.252378\n",
      "batch 570: loss 0.265885\n",
      "batch 571: loss 0.103608\n",
      "batch 572: loss 0.197934\n",
      "batch 573: loss 0.094086\n",
      "batch 574: loss 0.385640\n",
      "batch 575: loss 0.291395\n",
      "batch 576: loss 0.141477\n",
      "batch 577: loss 0.132113\n",
      "batch 578: loss 0.240027\n",
      "batch 579: loss 0.125857\n",
      "batch 580: loss 0.378013\n",
      "batch 581: loss 0.262841\n",
      "batch 582: loss 0.134967\n",
      "batch 583: loss 0.208452\n",
      "batch 584: loss 0.215710\n",
      "batch 585: loss 0.232886\n",
      "batch 586: loss 0.432822\n",
      "batch 587: loss 0.275294\n",
      "batch 588: loss 0.286309\n",
      "batch 589: loss 0.368830\n",
      "batch 590: loss 0.225502\n",
      "batch 591: loss 0.360639\n",
      "batch 592: loss 0.180124\n",
      "batch 593: loss 0.143306\n",
      "batch 594: loss 0.235702\n",
      "batch 595: loss 0.467001\n",
      "batch 596: loss 0.284193\n",
      "batch 597: loss 0.257383\n",
      "batch 598: loss 0.187873\n",
      "batch 599: loss 0.315942\n",
      "batch 600: loss 0.118981\n",
      "batch 601: loss 0.108817\n",
      "batch 602: loss 0.305343\n",
      "batch 603: loss 0.312117\n",
      "batch 604: loss 0.297807\n",
      "batch 605: loss 0.246653\n",
      "batch 606: loss 0.283954\n",
      "batch 607: loss 0.155502\n",
      "batch 608: loss 0.273219\n",
      "batch 609: loss 0.235053\n",
      "batch 610: loss 0.205708\n",
      "batch 611: loss 0.188939\n",
      "batch 612: loss 0.206331\n",
      "batch 613: loss 0.105243\n",
      "batch 614: loss 0.260127\n",
      "batch 615: loss 0.163936\n",
      "batch 616: loss 0.096036\n",
      "batch 617: loss 0.202946\n",
      "batch 618: loss 0.271792\n",
      "batch 619: loss 0.134733\n",
      "batch 620: loss 0.098759\n",
      "batch 621: loss 0.135999\n",
      "batch 622: loss 0.358175\n",
      "batch 623: loss 0.302541\n",
      "batch 624: loss 0.180129\n",
      "batch 625: loss 0.150227\n",
      "batch 626: loss 0.228491\n",
      "batch 627: loss 0.102399\n",
      "batch 628: loss 0.264887\n",
      "batch 629: loss 0.358476\n",
      "batch 630: loss 0.122095\n",
      "batch 631: loss 0.293349\n",
      "batch 632: loss 0.410796\n",
      "batch 633: loss 0.112282\n",
      "batch 634: loss 0.209284\n",
      "batch 635: loss 0.177152\n",
      "batch 636: loss 0.422367\n",
      "batch 637: loss 0.375822\n",
      "batch 638: loss 0.105856\n",
      "batch 639: loss 0.304973\n",
      "batch 640: loss 0.256160\n",
      "batch 641: loss 0.128454\n",
      "batch 642: loss 0.190885\n",
      "batch 643: loss 0.280894\n",
      "batch 644: loss 0.194705\n",
      "batch 645: loss 0.307275\n",
      "batch 646: loss 0.237806\n",
      "batch 647: loss 0.286113\n",
      "batch 648: loss 0.242038\n",
      "batch 649: loss 0.347168\n",
      "batch 650: loss 0.304677\n",
      "batch 651: loss 0.238716\n",
      "batch 652: loss 0.069405\n",
      "batch 653: loss 0.194364\n",
      "batch 654: loss 0.210522\n",
      "batch 655: loss 0.282765\n",
      "batch 656: loss 0.197528\n",
      "batch 657: loss 0.447420\n",
      "batch 658: loss 0.071807\n",
      "batch 659: loss 0.277550\n",
      "batch 660: loss 0.259500\n",
      "batch 661: loss 0.187024\n",
      "batch 662: loss 0.093910\n",
      "batch 663: loss 0.181171\n",
      "batch 664: loss 0.224677\n",
      "batch 665: loss 0.154490\n",
      "batch 666: loss 0.450258\n",
      "batch 667: loss 0.251034\n",
      "batch 668: loss 0.134176\n",
      "batch 669: loss 0.114775\n",
      "batch 670: loss 0.299894\n",
      "batch 671: loss 0.199422\n",
      "batch 672: loss 0.427695\n",
      "batch 673: loss 0.209387\n",
      "batch 674: loss 0.184765\n",
      "batch 675: loss 0.160763\n",
      "batch 676: loss 0.140246\n",
      "batch 677: loss 0.200386\n",
      "batch 678: loss 0.276182\n",
      "batch 679: loss 0.216938\n",
      "batch 680: loss 0.179126\n",
      "batch 681: loss 0.378322\n",
      "batch 682: loss 0.162520\n",
      "batch 683: loss 0.268968\n",
      "batch 684: loss 0.334894\n",
      "batch 685: loss 0.134805\n",
      "batch 686: loss 0.187228\n",
      "batch 687: loss 0.073204\n",
      "batch 688: loss 0.093184\n",
      "batch 689: loss 0.184306\n",
      "batch 690: loss 0.056683\n",
      "batch 691: loss 0.180389\n",
      "batch 692: loss 0.173023\n",
      "batch 693: loss 0.249112\n",
      "batch 694: loss 0.138666\n",
      "batch 695: loss 0.134486\n",
      "batch 696: loss 0.219430\n",
      "batch 697: loss 0.137832\n",
      "batch 698: loss 0.234068\n",
      "batch 699: loss 0.253757\n",
      "batch 700: loss 0.308590\n",
      "batch 701: loss 0.410141\n",
      "batch 702: loss 0.246874\n",
      "batch 703: loss 0.105812\n",
      "batch 704: loss 0.164933\n",
      "batch 705: loss 0.096107\n",
      "batch 706: loss 0.387094\n",
      "batch 707: loss 0.307830\n",
      "batch 708: loss 0.094511\n",
      "batch 709: loss 0.229460\n",
      "batch 710: loss 0.248869\n",
      "batch 711: loss 0.092441\n",
      "batch 712: loss 0.227630\n",
      "batch 713: loss 0.367959\n",
      "batch 714: loss 0.231157\n",
      "batch 715: loss 0.177956\n",
      "batch 716: loss 0.098090\n",
      "batch 717: loss 0.117870\n",
      "batch 718: loss 0.394364\n",
      "batch 719: loss 0.453735\n",
      "batch 720: loss 0.099923\n",
      "batch 721: loss 0.035748\n",
      "batch 722: loss 0.357474\n",
      "batch 723: loss 0.237649\n",
      "batch 724: loss 0.124772\n",
      "batch 725: loss 0.136265\n",
      "batch 726: loss 0.288523\n",
      "batch 727: loss 0.245899\n",
      "batch 728: loss 0.146418\n",
      "batch 729: loss 0.370323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 730: loss 0.229159\n",
      "batch 731: loss 0.096068\n",
      "batch 732: loss 0.288509\n",
      "batch 733: loss 0.139499\n",
      "batch 734: loss 0.177895\n",
      "batch 735: loss 0.255647\n",
      "batch 736: loss 0.202073\n",
      "batch 737: loss 0.091087\n",
      "batch 738: loss 0.155384\n",
      "batch 739: loss 0.218940\n",
      "batch 740: loss 0.191240\n",
      "batch 741: loss 0.142885\n",
      "batch 742: loss 0.102783\n",
      "batch 743: loss 0.208125\n",
      "batch 744: loss 0.234769\n",
      "batch 745: loss 0.220726\n",
      "batch 746: loss 0.198813\n",
      "batch 747: loss 0.228654\n",
      "batch 748: loss 0.148014\n",
      "batch 749: loss 0.235533\n",
      "batch 750: loss 0.115488\n",
      "batch 751: loss 0.335874\n",
      "batch 752: loss 0.226068\n",
      "batch 753: loss 0.106586\n",
      "batch 754: loss 0.183201\n",
      "batch 755: loss 0.206711\n",
      "batch 756: loss 0.160052\n",
      "batch 757: loss 0.117321\n",
      "batch 758: loss 0.176814\n",
      "batch 759: loss 0.321100\n",
      "batch 760: loss 0.389523\n",
      "batch 761: loss 0.114182\n",
      "batch 762: loss 0.234857\n",
      "batch 763: loss 0.214434\n",
      "batch 764: loss 0.104792\n",
      "batch 765: loss 0.035862\n",
      "batch 766: loss 0.146208\n",
      "batch 767: loss 0.102475\n",
      "batch 768: loss 0.127948\n",
      "batch 769: loss 0.358294\n",
      "batch 770: loss 0.250511\n",
      "batch 771: loss 0.144997\n",
      "batch 772: loss 0.185572\n",
      "batch 773: loss 0.348957\n",
      "batch 774: loss 0.234559\n",
      "batch 775: loss 0.152291\n",
      "batch 776: loss 0.285080\n",
      "batch 777: loss 0.218777\n",
      "batch 778: loss 0.332609\n",
      "batch 779: loss 0.131792\n",
      "batch 780: loss 0.242716\n",
      "batch 781: loss 0.240237\n",
      "batch 782: loss 0.234031\n",
      "batch 783: loss 0.048959\n",
      "batch 784: loss 0.300136\n",
      "batch 785: loss 0.065233\n",
      "batch 786: loss 0.210617\n",
      "batch 787: loss 0.374488\n",
      "batch 788: loss 0.121965\n",
      "batch 789: loss 0.150272\n",
      "batch 790: loss 0.249040\n",
      "batch 791: loss 0.192080\n",
      "batch 792: loss 0.301678\n",
      "batch 793: loss 0.073082\n",
      "batch 794: loss 0.091908\n",
      "batch 795: loss 0.311658\n",
      "batch 796: loss 0.231212\n",
      "batch 797: loss 0.102228\n",
      "batch 798: loss 0.139311\n",
      "batch 799: loss 0.233168\n",
      "batch 800: loss 0.307748\n",
      "batch 801: loss 0.128413\n",
      "batch 802: loss 0.194706\n",
      "batch 803: loss 0.493042\n",
      "batch 804: loss 0.139447\n",
      "batch 805: loss 0.147261\n",
      "batch 806: loss 0.203754\n",
      "batch 807: loss 0.230971\n",
      "batch 808: loss 0.199393\n",
      "batch 809: loss 0.196066\n",
      "batch 810: loss 0.282553\n",
      "batch 811: loss 0.171988\n",
      "batch 812: loss 0.462821\n",
      "batch 813: loss 0.369647\n",
      "batch 814: loss 0.072409\n",
      "batch 815: loss 0.175165\n",
      "batch 816: loss 0.223100\n",
      "batch 817: loss 0.363992\n",
      "batch 818: loss 0.286479\n",
      "batch 819: loss 0.189859\n",
      "batch 820: loss 0.095025\n",
      "batch 821: loss 0.198400\n",
      "batch 822: loss 0.104439\n",
      "batch 823: loss 0.162931\n",
      "batch 824: loss 0.150029\n",
      "batch 825: loss 0.165773\n",
      "batch 826: loss 0.184552\n",
      "batch 827: loss 0.173342\n",
      "batch 828: loss 0.141026\n",
      "batch 829: loss 0.144348\n",
      "batch 830: loss 0.174901\n",
      "batch 831: loss 0.195139\n",
      "batch 832: loss 0.208857\n",
      "batch 833: loss 0.158271\n",
      "batch 834: loss 0.161469\n",
      "batch 835: loss 0.261632\n",
      "batch 836: loss 0.115482\n",
      "batch 837: loss 0.082894\n",
      "batch 838: loss 0.074980\n",
      "batch 839: loss 0.235296\n",
      "batch 840: loss 0.179608\n",
      "batch 841: loss 0.390630\n",
      "batch 842: loss 0.302761\n",
      "batch 843: loss 0.151887\n",
      "batch 844: loss 0.069748\n",
      "batch 845: loss 0.289785\n",
      "batch 846: loss 0.106028\n",
      "batch 847: loss 0.108219\n",
      "batch 848: loss 0.171101\n",
      "batch 849: loss 0.164531\n",
      "batch 850: loss 0.280475\n",
      "batch 851: loss 0.057210\n",
      "batch 852: loss 0.195725\n",
      "batch 853: loss 0.213229\n",
      "batch 854: loss 0.193627\n",
      "batch 855: loss 0.210985\n",
      "batch 856: loss 0.287082\n",
      "batch 857: loss 0.226602\n",
      "batch 858: loss 0.110118\n",
      "batch 859: loss 0.149915\n",
      "batch 860: loss 0.335437\n",
      "batch 861: loss 0.099223\n",
      "batch 862: loss 0.588329\n",
      "batch 863: loss 0.071966\n",
      "batch 864: loss 0.299240\n",
      "batch 865: loss 0.200620\n",
      "batch 866: loss 0.233196\n",
      "batch 867: loss 0.120359\n",
      "batch 868: loss 0.132210\n",
      "batch 869: loss 0.281969\n",
      "batch 870: loss 0.379935\n",
      "batch 871: loss 0.173481\n",
      "batch 872: loss 0.252911\n",
      "batch 873: loss 0.092524\n",
      "batch 874: loss 0.316588\n",
      "batch 875: loss 0.387795\n",
      "batch 876: loss 0.307719\n",
      "batch 877: loss 0.086978\n",
      "batch 878: loss 0.281222\n",
      "batch 879: loss 0.195454\n",
      "batch 880: loss 0.107105\n",
      "batch 881: loss 0.112637\n",
      "batch 882: loss 0.416165\n",
      "batch 883: loss 0.172478\n",
      "batch 884: loss 0.130058\n",
      "batch 885: loss 0.152702\n",
      "batch 886: loss 0.300262\n",
      "batch 887: loss 0.188537\n",
      "batch 888: loss 0.268929\n",
      "batch 889: loss 0.196782\n",
      "batch 890: loss 0.150911\n",
      "batch 891: loss 0.305887\n",
      "batch 892: loss 0.145874\n",
      "batch 893: loss 0.269511\n",
      "batch 894: loss 0.312436\n",
      "batch 895: loss 0.114915\n",
      "batch 896: loss 0.351759\n",
      "batch 897: loss 0.144289\n",
      "batch 898: loss 0.078480\n",
      "batch 899: loss 0.274063\n",
      "batch 900: loss 0.107378\n",
      "batch 901: loss 0.297562\n",
      "batch 902: loss 0.207935\n",
      "batch 903: loss 0.245902\n",
      "batch 904: loss 0.173413\n",
      "batch 905: loss 0.218707\n",
      "batch 906: loss 0.226495\n",
      "batch 907: loss 0.235985\n",
      "batch 908: loss 0.121681\n",
      "batch 909: loss 0.255076\n",
      "batch 910: loss 0.086359\n",
      "batch 911: loss 0.095212\n",
      "batch 912: loss 0.080241\n",
      "batch 913: loss 0.094393\n",
      "batch 914: loss 0.048338\n",
      "batch 915: loss 0.233525\n",
      "batch 916: loss 0.201733\n",
      "batch 917: loss 0.222120\n",
      "batch 918: loss 0.087064\n",
      "batch 919: loss 0.095579\n",
      "batch 920: loss 0.120347\n",
      "batch 921: loss 0.153205\n",
      "batch 922: loss 0.118438\n",
      "batch 923: loss 0.138037\n",
      "batch 924: loss 0.127277\n",
      "batch 925: loss 0.094739\n",
      "batch 926: loss 0.175657\n",
      "batch 927: loss 0.187509\n",
      "batch 928: loss 0.135583\n",
      "batch 929: loss 0.254480\n",
      "batch 930: loss 0.208474\n",
      "batch 931: loss 0.214135\n",
      "batch 932: loss 0.193623\n",
      "batch 933: loss 0.192170\n",
      "batch 934: loss 0.139883\n",
      "batch 935: loss 0.116621\n",
      "batch 936: loss 0.136729\n",
      "batch 937: loss 0.159938\n",
      "batch 938: loss 0.308853\n",
      "batch 939: loss 0.247106\n",
      "batch 940: loss 0.090695\n",
      "batch 941: loss 0.090985\n",
      "batch 942: loss 0.154181\n",
      "batch 943: loss 0.242249\n",
      "batch 944: loss 0.271931\n",
      "batch 945: loss 0.227896\n",
      "batch 946: loss 0.102871\n",
      "batch 947: loss 0.215802\n",
      "batch 948: loss 0.193968\n",
      "batch 949: loss 0.053627\n",
      "batch 950: loss 0.083614\n",
      "batch 951: loss 0.032942\n",
      "batch 952: loss 0.161953\n",
      "batch 953: loss 0.101224\n",
      "batch 954: loss 0.173647\n",
      "batch 955: loss 0.271450\n",
      "batch 956: loss 0.060215\n",
      "batch 957: loss 0.161726\n",
      "batch 958: loss 0.238862\n",
      "batch 959: loss 0.300739\n",
      "batch 960: loss 0.143460\n",
      "batch 961: loss 0.249059\n",
      "batch 962: loss 0.120851\n",
      "batch 963: loss 0.095560\n",
      "batch 964: loss 0.210529\n",
      "batch 965: loss 0.238026\n",
      "batch 966: loss 0.230056\n",
      "batch 967: loss 0.201059\n",
      "batch 968: loss 0.094195\n",
      "batch 969: loss 0.130224\n",
      "batch 970: loss 0.160705\n",
      "batch 971: loss 0.074676\n",
      "batch 972: loss 0.161111\n",
      "batch 973: loss 0.085418\n",
      "batch 974: loss 0.125101\n",
      "batch 975: loss 0.085320\n",
      "batch 976: loss 0.132461\n",
      "batch 977: loss 0.225719\n",
      "batch 978: loss 0.269064\n",
      "batch 979: loss 0.211777\n",
      "batch 980: loss 0.170881\n",
      "batch 981: loss 0.065793\n",
      "batch 982: loss 0.143955\n",
      "batch 983: loss 0.111375\n",
      "batch 984: loss 0.134832\n",
      "batch 985: loss 0.101903\n",
      "batch 986: loss 0.129763\n",
      "batch 987: loss 0.175560\n",
      "batch 988: loss 0.132163\n",
      "batch 989: loss 0.059558\n",
      "batch 990: loss 0.141694\n",
      "batch 991: loss 0.120462\n",
      "batch 992: loss 0.123080\n",
      "batch 993: loss 0.096002\n",
      "batch 994: loss 0.046220\n",
      "batch 995: loss 0.177961\n",
      "batch 996: loss 0.141296\n",
      "batch 997: loss 0.102700\n",
      "batch 998: loss 0.051722\n",
      "batch 999: loss 0.088679\n"
     ]
    }
   ],
   "source": [
    "summary_writer = tf.summary.create_file_writer('./tensorboard')     # 实例化记录器\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "        with summary_writer.as_default():                           # 指定记录器\n",
    "            tf.summary.scalar(\"loss\", loss, step=batch_index)       # 将当前损失函数的值写入记录器\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the direct of the tensorboard.py and use the commanda: tensorboard PATH --logdir logDIRECTORY. Open the directory rather than the file!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
