{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "        # MNIST中的图像默认为uint8（0-255的数字）。以下代码将其归一化到0-1之间的浮点数，并在最后增加一维作为颜色通道\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        # 从数据集中随机取出batch_size个元素并返回\n",
    "        index = np.random.randint(0, np.shape(self.train_data)[0], batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.flatten(inputs)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        outputs = tf.nn.softmax(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 1000\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "model = MLP()\n",
    "data_loader = MNISTLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例化记录器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.345323\n",
      "batch 1: loss 2.254747\n",
      "batch 2: loss 2.207098\n",
      "batch 3: loss 2.241024\n",
      "batch 4: loss 2.019653\n",
      "batch 5: loss 2.001518\n",
      "batch 6: loss 1.890124\n",
      "batch 7: loss 1.912870\n",
      "batch 8: loss 1.769376\n",
      "batch 9: loss 1.766419\n",
      "batch 10: loss 1.787181\n",
      "batch 11: loss 1.632567\n",
      "batch 12: loss 1.682978\n",
      "batch 13: loss 1.478029\n",
      "batch 14: loss 1.595841\n",
      "batch 15: loss 1.443416\n",
      "batch 16: loss 1.427969\n",
      "batch 17: loss 1.343017\n",
      "batch 18: loss 1.263431\n",
      "batch 19: loss 1.273728\n",
      "batch 20: loss 1.151004\n",
      "batch 21: loss 1.160743\n",
      "batch 22: loss 1.008276\n",
      "batch 23: loss 1.231272\n",
      "batch 24: loss 1.140312\n",
      "batch 25: loss 1.152695\n",
      "batch 26: loss 0.856181\n",
      "batch 27: loss 1.056972\n",
      "batch 28: loss 0.947077\n",
      "batch 29: loss 0.801167\n",
      "batch 30: loss 0.871780\n",
      "batch 31: loss 0.786483\n",
      "batch 32: loss 0.870514\n",
      "batch 33: loss 0.747699\n",
      "batch 34: loss 0.787235\n",
      "batch 35: loss 0.914949\n",
      "batch 36: loss 0.723356\n",
      "batch 37: loss 0.686948\n",
      "batch 38: loss 0.648106\n",
      "batch 39: loss 0.692099\n",
      "batch 40: loss 0.712926\n",
      "batch 41: loss 0.678926\n",
      "batch 42: loss 0.579735\n",
      "batch 43: loss 0.453063\n",
      "batch 44: loss 0.598719\n",
      "batch 45: loss 0.567740\n",
      "batch 46: loss 0.794382\n",
      "batch 47: loss 0.642299\n",
      "batch 48: loss 0.553714\n",
      "batch 49: loss 0.557792\n",
      "batch 50: loss 0.620669\n",
      "batch 51: loss 0.622052\n",
      "batch 52: loss 0.710338\n",
      "batch 53: loss 0.624581\n",
      "batch 54: loss 0.732913\n",
      "batch 55: loss 0.596646\n",
      "batch 56: loss 0.507229\n",
      "batch 57: loss 0.649405\n",
      "batch 58: loss 0.549475\n",
      "batch 59: loss 0.634168\n",
      "batch 60: loss 0.509230\n",
      "batch 61: loss 0.551924\n",
      "batch 62: loss 0.615646\n",
      "batch 63: loss 0.553318\n",
      "batch 64: loss 0.623725\n",
      "batch 65: loss 0.551515\n",
      "batch 66: loss 0.387781\n",
      "batch 67: loss 0.407228\n",
      "batch 68: loss 0.460874\n",
      "batch 69: loss 0.578149\n",
      "batch 70: loss 0.422552\n",
      "batch 71: loss 0.419327\n",
      "batch 72: loss 0.547582\n",
      "batch 73: loss 0.574344\n",
      "batch 74: loss 0.695682\n",
      "batch 75: loss 0.578451\n",
      "batch 76: loss 0.498763\n",
      "batch 77: loss 0.517325\n",
      "batch 78: loss 0.433633\n",
      "batch 79: loss 0.623353\n",
      "batch 80: loss 0.431924\n",
      "batch 81: loss 0.514423\n",
      "batch 82: loss 0.473019\n",
      "batch 83: loss 0.394556\n",
      "batch 84: loss 0.463969\n",
      "batch 85: loss 0.367758\n",
      "batch 86: loss 0.410729\n",
      "batch 87: loss 0.329768\n",
      "batch 88: loss 0.372678\n",
      "batch 89: loss 0.829700\n",
      "batch 90: loss 0.401187\n",
      "batch 91: loss 0.381396\n",
      "batch 92: loss 0.336050\n",
      "batch 93: loss 0.509276\n",
      "batch 94: loss 0.466584\n",
      "batch 95: loss 0.377726\n",
      "batch 96: loss 0.554255\n",
      "batch 97: loss 0.532170\n",
      "batch 98: loss 0.299519\n",
      "batch 99: loss 0.348658\n",
      "batch 100: loss 0.447495\n",
      "batch 101: loss 0.399742\n",
      "batch 102: loss 0.406377\n",
      "batch 103: loss 0.516873\n",
      "batch 104: loss 0.437362\n",
      "batch 105: loss 0.588262\n",
      "batch 106: loss 0.286017\n",
      "batch 107: loss 0.481336\n",
      "batch 108: loss 0.410794\n",
      "batch 109: loss 0.391482\n",
      "batch 110: loss 0.389607\n",
      "batch 111: loss 0.381619\n",
      "batch 112: loss 0.545828\n",
      "batch 113: loss 0.423318\n",
      "batch 114: loss 0.543643\n",
      "batch 115: loss 0.471701\n",
      "batch 116: loss 0.409428\n",
      "batch 117: loss 0.323247\n",
      "batch 118: loss 0.476181\n",
      "batch 119: loss 0.521679\n",
      "batch 120: loss 0.422219\n",
      "batch 121: loss 0.442604\n",
      "batch 122: loss 0.194649\n",
      "batch 123: loss 0.386315\n",
      "batch 124: loss 0.294917\n",
      "batch 125: loss 0.306243\n",
      "batch 126: loss 0.505979\n",
      "batch 127: loss 0.398954\n",
      "batch 128: loss 0.599003\n",
      "batch 129: loss 0.524029\n",
      "batch 130: loss 0.444206\n",
      "batch 131: loss 0.548447\n",
      "batch 132: loss 0.351281\n",
      "batch 133: loss 0.488220\n",
      "batch 134: loss 0.343808\n",
      "batch 135: loss 0.397354\n",
      "batch 136: loss 0.561544\n",
      "batch 137: loss 0.372401\n",
      "batch 138: loss 0.336794\n",
      "batch 139: loss 0.360911\n",
      "batch 140: loss 0.311400\n",
      "batch 141: loss 0.363876\n",
      "batch 142: loss 0.321305\n",
      "batch 143: loss 0.501578\n",
      "batch 144: loss 0.244529\n",
      "batch 145: loss 0.518807\n",
      "batch 146: loss 0.373071\n",
      "batch 147: loss 0.423512\n",
      "batch 148: loss 0.274881\n",
      "batch 149: loss 0.371809\n",
      "batch 150: loss 0.356888\n",
      "batch 151: loss 0.369338\n",
      "batch 152: loss 0.520897\n",
      "batch 153: loss 0.250693\n",
      "batch 154: loss 0.219839\n",
      "batch 155: loss 0.280698\n",
      "batch 156: loss 0.255303\n",
      "batch 157: loss 0.282291\n",
      "batch 158: loss 0.295888\n",
      "batch 159: loss 0.541753\n",
      "batch 160: loss 0.325418\n",
      "batch 161: loss 0.265391\n",
      "batch 162: loss 0.286048\n",
      "batch 163: loss 0.477166\n",
      "batch 164: loss 0.344137\n",
      "batch 165: loss 0.203448\n",
      "batch 166: loss 0.674931\n",
      "batch 167: loss 0.363389\n",
      "batch 168: loss 0.439022\n",
      "batch 169: loss 0.190984\n",
      "batch 170: loss 0.335099\n",
      "batch 171: loss 0.117297\n",
      "batch 172: loss 0.361391\n",
      "batch 173: loss 0.379655\n",
      "batch 174: loss 0.402552\n",
      "batch 175: loss 0.443091\n",
      "batch 176: loss 0.321321\n",
      "batch 177: loss 0.242254\n",
      "batch 178: loss 0.337652\n",
      "batch 179: loss 0.392261\n",
      "batch 180: loss 0.518293\n",
      "batch 181: loss 0.577787\n",
      "batch 182: loss 0.164584\n",
      "batch 183: loss 0.254847\n",
      "batch 184: loss 0.357715\n",
      "batch 185: loss 0.334391\n",
      "batch 186: loss 0.300058\n",
      "batch 187: loss 0.193420\n",
      "batch 188: loss 0.394292\n",
      "batch 189: loss 0.332852\n",
      "batch 190: loss 0.522868\n",
      "batch 191: loss 0.263088\n",
      "batch 192: loss 0.356037\n",
      "batch 193: loss 0.277287\n",
      "batch 194: loss 0.573912\n",
      "batch 195: loss 0.325240\n",
      "batch 196: loss 0.317732\n",
      "batch 197: loss 0.331470\n",
      "batch 198: loss 0.425751\n",
      "batch 199: loss 0.245154\n",
      "batch 200: loss 0.408675\n",
      "batch 201: loss 0.335047\n",
      "batch 202: loss 0.208858\n",
      "batch 203: loss 0.218481\n",
      "batch 204: loss 0.381302\n",
      "batch 205: loss 0.272547\n",
      "batch 206: loss 0.312336\n",
      "batch 207: loss 0.277253\n",
      "batch 208: loss 0.270783\n",
      "batch 209: loss 0.246645\n",
      "batch 210: loss 0.358052\n",
      "batch 211: loss 0.511603\n",
      "batch 212: loss 0.534836\n",
      "batch 213: loss 0.180341\n",
      "batch 214: loss 0.442401\n",
      "batch 215: loss 0.318140\n",
      "batch 216: loss 0.302355\n",
      "batch 217: loss 0.284726\n",
      "batch 218: loss 0.225498\n",
      "batch 219: loss 0.391009\n",
      "batch 220: loss 0.404530\n",
      "batch 221: loss 0.171710\n",
      "batch 222: loss 0.179442\n",
      "batch 223: loss 0.462661\n",
      "batch 224: loss 0.608114\n",
      "batch 225: loss 0.403093\n",
      "batch 226: loss 0.328449\n",
      "batch 227: loss 0.215379\n",
      "batch 228: loss 0.267788\n",
      "batch 229: loss 0.196872\n",
      "batch 230: loss 0.295885\n",
      "batch 231: loss 0.700594\n",
      "batch 232: loss 0.259166\n",
      "batch 233: loss 0.216888\n",
      "batch 234: loss 0.213151\n",
      "batch 235: loss 0.240971\n",
      "batch 236: loss 0.320275\n",
      "batch 237: loss 0.303377\n",
      "batch 238: loss 0.290736\n",
      "batch 239: loss 0.266424\n",
      "batch 240: loss 0.184146\n",
      "batch 241: loss 0.169137\n",
      "batch 242: loss 0.242706\n",
      "batch 243: loss 0.256244\n",
      "batch 244: loss 0.172015\n",
      "batch 245: loss 0.196444\n",
      "batch 246: loss 0.238696\n",
      "batch 247: loss 0.229875\n",
      "batch 248: loss 0.505704\n",
      "batch 249: loss 0.349326\n",
      "batch 250: loss 0.411943\n",
      "batch 251: loss 0.193210\n",
      "batch 252: loss 0.221607\n",
      "batch 253: loss 0.204734\n",
      "batch 254: loss 0.202674\n",
      "batch 255: loss 0.264572\n",
      "batch 256: loss 0.441474\n",
      "batch 257: loss 0.277957\n",
      "batch 258: loss 0.344093\n",
      "batch 259: loss 0.259812\n",
      "batch 260: loss 0.304470\n",
      "batch 261: loss 0.365828\n",
      "batch 262: loss 0.184600\n",
      "batch 263: loss 0.301735\n",
      "batch 264: loss 0.307006\n",
      "batch 265: loss 0.240301\n",
      "batch 266: loss 0.335734\n",
      "batch 267: loss 0.269114\n",
      "batch 268: loss 0.316009\n",
      "batch 269: loss 0.373677\n",
      "batch 270: loss 0.322158\n",
      "batch 271: loss 0.300468\n",
      "batch 272: loss 0.207473\n",
      "batch 273: loss 0.274341\n",
      "batch 274: loss 0.455928\n",
      "batch 275: loss 0.393999\n",
      "batch 276: loss 0.289860\n",
      "batch 277: loss 0.663490\n",
      "batch 278: loss 0.337453\n",
      "batch 279: loss 0.481573\n",
      "batch 280: loss 0.340737\n",
      "batch 281: loss 0.363961\n",
      "batch 282: loss 0.165096\n",
      "batch 283: loss 0.202788\n",
      "batch 284: loss 0.250765\n",
      "batch 285: loss 0.159879\n",
      "batch 286: loss 0.231677\n",
      "batch 287: loss 0.174524\n",
      "batch 288: loss 0.218704\n",
      "batch 289: loss 0.331380\n",
      "batch 290: loss 0.076088\n",
      "batch 291: loss 0.372307\n",
      "batch 292: loss 0.108735\n",
      "batch 293: loss 0.261745\n",
      "batch 294: loss 0.147534\n",
      "batch 295: loss 0.542828\n",
      "batch 296: loss 0.447086\n",
      "batch 297: loss 0.289400\n",
      "batch 298: loss 0.284567\n",
      "batch 299: loss 0.127157\n",
      "batch 300: loss 0.105440\n",
      "batch 301: loss 0.288558\n",
      "batch 302: loss 0.151420\n",
      "batch 303: loss 0.151559\n",
      "batch 304: loss 0.336391\n",
      "batch 305: loss 0.160199\n",
      "batch 306: loss 0.316248\n",
      "batch 307: loss 0.194123\n",
      "batch 308: loss 0.410155\n",
      "batch 309: loss 0.335677\n",
      "batch 310: loss 0.250959\n",
      "batch 311: loss 0.579334\n",
      "batch 312: loss 0.281558\n",
      "batch 313: loss 0.165165\n",
      "batch 314: loss 0.202048\n",
      "batch 315: loss 0.352692\n",
      "batch 316: loss 0.260512\n",
      "batch 317: loss 0.322843\n",
      "batch 318: loss 0.312426\n",
      "batch 319: loss 0.198340\n",
      "batch 320: loss 0.280266\n",
      "batch 321: loss 0.232546\n",
      "batch 322: loss 0.283090\n",
      "batch 323: loss 0.472697\n",
      "batch 324: loss 0.264083\n",
      "batch 325: loss 0.379063\n",
      "batch 326: loss 0.192393\n",
      "batch 327: loss 0.103165\n",
      "batch 328: loss 0.149743\n",
      "batch 329: loss 0.432021\n",
      "batch 330: loss 0.433643\n",
      "batch 331: loss 0.233457\n",
      "batch 332: loss 0.318045\n",
      "batch 333: loss 0.499328\n",
      "batch 334: loss 0.128573\n",
      "batch 335: loss 0.403289\n",
      "batch 336: loss 0.178211\n",
      "batch 337: loss 0.503735\n",
      "batch 338: loss 0.728813\n",
      "batch 339: loss 0.237608\n",
      "batch 340: loss 0.258770\n",
      "batch 341: loss 0.222917\n",
      "batch 342: loss 0.243803\n",
      "batch 343: loss 0.320656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 344: loss 0.348189\n",
      "batch 345: loss 0.222927\n",
      "batch 346: loss 0.595960\n",
      "batch 347: loss 0.187787\n",
      "batch 348: loss 0.280682\n",
      "batch 349: loss 0.255645\n",
      "batch 350: loss 0.178113\n",
      "batch 351: loss 0.477101\n",
      "batch 352: loss 0.215118\n",
      "batch 353: loss 0.362837\n",
      "batch 354: loss 0.382238\n",
      "batch 355: loss 0.399054\n",
      "batch 356: loss 0.399443\n",
      "batch 357: loss 0.086055\n",
      "batch 358: loss 0.240005\n",
      "batch 359: loss 0.276523\n",
      "batch 360: loss 0.248441\n",
      "batch 361: loss 0.135649\n",
      "batch 362: loss 0.351568\n",
      "batch 363: loss 0.139059\n",
      "batch 364: loss 0.366439\n",
      "batch 365: loss 0.425473\n",
      "batch 366: loss 0.210115\n",
      "batch 367: loss 0.216705\n",
      "batch 368: loss 0.185757\n",
      "batch 369: loss 0.211754\n",
      "batch 370: loss 0.238598\n",
      "batch 371: loss 0.337253\n",
      "batch 372: loss 0.205219\n",
      "batch 373: loss 0.559156\n",
      "batch 374: loss 0.315772\n",
      "batch 375: loss 0.330961\n",
      "batch 376: loss 0.340124\n",
      "batch 377: loss 0.430499\n",
      "batch 378: loss 0.196450\n",
      "batch 379: loss 0.181098\n",
      "batch 380: loss 0.313292\n",
      "batch 381: loss 0.394156\n",
      "batch 382: loss 0.201023\n",
      "batch 383: loss 0.501375\n",
      "batch 384: loss 0.204738\n",
      "batch 385: loss 0.166518\n",
      "batch 386: loss 0.150127\n",
      "batch 387: loss 0.233314\n",
      "batch 388: loss 0.208507\n",
      "batch 389: loss 0.180027\n",
      "batch 390: loss 0.467896\n",
      "batch 391: loss 0.329393\n",
      "batch 392: loss 0.155575\n",
      "batch 393: loss 0.256267\n",
      "batch 394: loss 0.407888\n",
      "batch 395: loss 0.180205\n",
      "batch 396: loss 0.428900\n",
      "batch 397: loss 0.259690\n",
      "batch 398: loss 0.163873\n",
      "batch 399: loss 0.438768\n",
      "batch 400: loss 0.226846\n",
      "batch 401: loss 0.363622\n",
      "batch 402: loss 0.084689\n",
      "batch 403: loss 0.204646\n",
      "batch 404: loss 0.162989\n",
      "batch 405: loss 0.371610\n",
      "batch 406: loss 0.202591\n",
      "batch 407: loss 0.383548\n",
      "batch 408: loss 0.383421\n",
      "batch 409: loss 0.286316\n",
      "batch 410: loss 0.211457\n",
      "batch 411: loss 0.236432\n",
      "batch 412: loss 0.316477\n",
      "batch 413: loss 0.176213\n",
      "batch 414: loss 0.132464\n",
      "batch 415: loss 0.345287\n",
      "batch 416: loss 0.150986\n",
      "batch 417: loss 0.245266\n",
      "batch 418: loss 0.177881\n",
      "batch 419: loss 0.242878\n",
      "batch 420: loss 0.397576\n",
      "batch 421: loss 0.418029\n",
      "batch 422: loss 0.327698\n",
      "batch 423: loss 0.146014\n",
      "batch 424: loss 0.147459\n",
      "batch 425: loss 0.351740\n",
      "batch 426: loss 0.145242\n",
      "batch 427: loss 0.250387\n",
      "batch 428: loss 0.179417\n",
      "batch 429: loss 0.283819\n",
      "batch 430: loss 0.298124\n",
      "batch 431: loss 0.314057\n",
      "batch 432: loss 0.260997\n",
      "batch 433: loss 0.679239\n",
      "batch 434: loss 0.143724\n",
      "batch 435: loss 0.271948\n",
      "batch 436: loss 0.232478\n",
      "batch 437: loss 0.349126\n",
      "batch 438: loss 0.145603\n",
      "batch 439: loss 0.175945\n",
      "batch 440: loss 0.283904\n",
      "batch 441: loss 0.283591\n",
      "batch 442: loss 0.293045\n",
      "batch 443: loss 0.324824\n",
      "batch 444: loss 0.181670\n",
      "batch 445: loss 0.136705\n",
      "batch 446: loss 0.140309\n",
      "batch 447: loss 0.225931\n",
      "batch 448: loss 0.274595\n",
      "batch 449: loss 0.300484\n",
      "batch 450: loss 0.065824\n",
      "batch 451: loss 0.226567\n",
      "batch 452: loss 0.095996\n",
      "batch 453: loss 0.283918\n",
      "batch 454: loss 0.276002\n",
      "batch 455: loss 0.304452\n",
      "batch 456: loss 0.250479\n",
      "batch 457: loss 0.247368\n",
      "batch 458: loss 0.165238\n",
      "batch 459: loss 0.276986\n",
      "batch 460: loss 0.191489\n",
      "batch 461: loss 0.231777\n",
      "batch 462: loss 0.272507\n",
      "batch 463: loss 0.259868\n",
      "batch 464: loss 0.240368\n",
      "batch 465: loss 0.373371\n",
      "batch 466: loss 0.318952\n",
      "batch 467: loss 0.163781\n",
      "batch 468: loss 0.245392\n",
      "batch 469: loss 0.272022\n",
      "batch 470: loss 0.224572\n",
      "batch 471: loss 0.166297\n",
      "batch 472: loss 0.280216\n",
      "batch 473: loss 0.385504\n",
      "batch 474: loss 0.544312\n",
      "batch 475: loss 0.199454\n",
      "batch 476: loss 0.247809\n",
      "batch 477: loss 0.215556\n",
      "batch 478: loss 0.245614\n",
      "batch 479: loss 0.315597\n",
      "batch 480: loss 0.399625\n",
      "batch 481: loss 0.131037\n",
      "batch 482: loss 0.203705\n",
      "batch 483: loss 0.337393\n",
      "batch 484: loss 0.325910\n",
      "batch 485: loss 0.172983\n",
      "batch 486: loss 0.282101\n",
      "batch 487: loss 0.212665\n",
      "batch 488: loss 0.112726\n",
      "batch 489: loss 0.161720\n",
      "batch 490: loss 0.140201\n",
      "batch 491: loss 0.171960\n",
      "batch 492: loss 0.217080\n",
      "batch 493: loss 0.452704\n",
      "batch 494: loss 0.215262\n",
      "batch 495: loss 0.113430\n",
      "batch 496: loss 0.324404\n",
      "batch 497: loss 0.418486\n",
      "batch 498: loss 0.232858\n",
      "batch 499: loss 0.131458\n",
      "batch 500: loss 0.453904\n",
      "batch 501: loss 0.216225\n",
      "batch 502: loss 0.172481\n",
      "batch 503: loss 0.311228\n",
      "batch 504: loss 0.251645\n",
      "batch 505: loss 0.139630\n",
      "batch 506: loss 0.455874\n",
      "batch 507: loss 0.135584\n",
      "batch 508: loss 0.469719\n",
      "batch 509: loss 0.208459\n",
      "batch 510: loss 0.216033\n",
      "batch 511: loss 0.141242\n",
      "batch 512: loss 0.126087\n",
      "batch 513: loss 0.128859\n",
      "batch 514: loss 0.347181\n",
      "batch 515: loss 0.128286\n",
      "batch 516: loss 0.251013\n",
      "batch 517: loss 0.174095\n",
      "batch 518: loss 0.428324\n",
      "batch 519: loss 0.145992\n",
      "batch 520: loss 0.226944\n",
      "batch 521: loss 0.182342\n",
      "batch 522: loss 0.333442\n",
      "batch 523: loss 0.083106\n",
      "batch 524: loss 0.097019\n",
      "batch 525: loss 0.178153\n",
      "batch 526: loss 0.170352\n",
      "batch 527: loss 0.386835\n",
      "batch 528: loss 0.115820\n",
      "batch 529: loss 0.374307\n",
      "batch 530: loss 0.402379\n",
      "batch 531: loss 0.572671\n",
      "batch 532: loss 0.272499\n",
      "batch 533: loss 0.254947\n",
      "batch 534: loss 0.264464\n",
      "batch 535: loss 0.287758\n",
      "batch 536: loss 0.100778\n",
      "batch 537: loss 0.281347\n",
      "batch 538: loss 0.317179\n",
      "batch 539: loss 0.111225\n",
      "batch 540: loss 0.352901\n",
      "batch 541: loss 0.333449\n",
      "batch 542: loss 0.117928\n",
      "batch 543: loss 0.132145\n",
      "batch 544: loss 0.171450\n",
      "batch 545: loss 0.192194\n",
      "batch 546: loss 0.272696\n",
      "batch 547: loss 0.262817\n",
      "batch 548: loss 0.126584\n",
      "batch 549: loss 0.066965\n",
      "batch 550: loss 0.114239\n",
      "batch 551: loss 0.410962\n",
      "batch 552: loss 0.155548\n",
      "batch 553: loss 0.137306\n",
      "batch 554: loss 0.337856\n",
      "batch 555: loss 0.274230\n",
      "batch 556: loss 0.135757\n",
      "batch 557: loss 0.127042\n",
      "batch 558: loss 0.269302\n",
      "batch 559: loss 0.257887\n",
      "batch 560: loss 0.196072\n",
      "batch 561: loss 0.182229\n",
      "batch 562: loss 0.225898\n",
      "batch 563: loss 0.206514\n",
      "batch 564: loss 0.210336\n",
      "batch 565: loss 0.387369\n",
      "batch 566: loss 0.222447\n",
      "batch 567: loss 0.285423\n",
      "batch 568: loss 0.259492\n",
      "batch 569: loss 0.299334\n",
      "batch 570: loss 0.298676\n",
      "batch 571: loss 0.239286\n",
      "batch 572: loss 0.258123\n",
      "batch 573: loss 0.249170\n",
      "batch 574: loss 0.208481\n",
      "batch 575: loss 0.196685\n",
      "batch 576: loss 0.168779\n",
      "batch 577: loss 0.248458\n",
      "batch 578: loss 0.217412\n",
      "batch 579: loss 0.178467\n",
      "batch 580: loss 0.229258\n",
      "batch 581: loss 0.122097\n",
      "batch 582: loss 0.347560\n",
      "batch 583: loss 0.117903\n",
      "batch 584: loss 0.178676\n",
      "batch 585: loss 0.170333\n",
      "batch 586: loss 0.326537\n",
      "batch 587: loss 0.172183\n",
      "batch 588: loss 0.242923\n",
      "batch 589: loss 0.251298\n",
      "batch 590: loss 0.084545\n",
      "batch 591: loss 0.708117\n",
      "batch 592: loss 0.143322\n",
      "batch 593: loss 0.297724\n",
      "batch 594: loss 0.210226\n",
      "batch 595: loss 0.329087\n",
      "batch 596: loss 0.119557\n",
      "batch 597: loss 0.471478\n",
      "batch 598: loss 0.351656\n",
      "batch 599: loss 0.185177\n",
      "batch 600: loss 0.217960\n",
      "batch 601: loss 0.272489\n",
      "batch 602: loss 0.324211\n",
      "batch 603: loss 0.224978\n",
      "batch 604: loss 0.093622\n",
      "batch 605: loss 0.156661\n",
      "batch 606: loss 0.131054\n",
      "batch 607: loss 0.284584\n",
      "batch 608: loss 0.143221\n",
      "batch 609: loss 0.154439\n",
      "batch 610: loss 0.134524\n",
      "batch 611: loss 0.240307\n",
      "batch 612: loss 0.370795\n",
      "batch 613: loss 0.422694\n",
      "batch 614: loss 0.185112\n",
      "batch 615: loss 0.627922\n",
      "batch 616: loss 0.225832\n",
      "batch 617: loss 0.229658\n",
      "batch 618: loss 0.265016\n",
      "batch 619: loss 0.125069\n",
      "batch 620: loss 0.137491\n",
      "batch 621: loss 0.267299\n",
      "batch 622: loss 0.142843\n",
      "batch 623: loss 0.160614\n",
      "batch 624: loss 0.245876\n",
      "batch 625: loss 0.151376\n",
      "batch 626: loss 0.172365\n",
      "batch 627: loss 0.249775\n",
      "batch 628: loss 0.303750\n",
      "batch 629: loss 0.175364\n",
      "batch 630: loss 0.327223\n",
      "batch 631: loss 0.084902\n",
      "batch 632: loss 0.303336\n",
      "batch 633: loss 0.081077\n",
      "batch 634: loss 0.209567\n",
      "batch 635: loss 0.333037\n",
      "batch 636: loss 0.209262\n",
      "batch 637: loss 0.147237\n",
      "batch 638: loss 0.139461\n",
      "batch 639: loss 0.251855\n",
      "batch 640: loss 0.111532\n",
      "batch 641: loss 0.320665\n",
      "batch 642: loss 0.293731\n",
      "batch 643: loss 0.293138\n",
      "batch 644: loss 0.280042\n",
      "batch 645: loss 0.269754\n",
      "batch 646: loss 0.363972\n",
      "batch 647: loss 0.173508\n",
      "batch 648: loss 0.307937\n",
      "batch 649: loss 0.142080\n",
      "batch 650: loss 0.378223\n",
      "batch 651: loss 0.442345\n",
      "batch 652: loss 0.356959\n",
      "batch 653: loss 0.356038\n",
      "batch 654: loss 0.264946\n",
      "batch 655: loss 0.341003\n",
      "batch 656: loss 0.163244\n",
      "batch 657: loss 0.197912\n",
      "batch 658: loss 0.224511\n",
      "batch 659: loss 0.441992\n",
      "batch 660: loss 0.170381\n",
      "batch 661: loss 0.248776\n",
      "batch 662: loss 0.167683\n",
      "batch 663: loss 0.325089\n",
      "batch 664: loss 0.146489\n",
      "batch 665: loss 0.198290\n",
      "batch 666: loss 0.288289\n",
      "batch 667: loss 0.265555\n",
      "batch 668: loss 0.203580\n",
      "batch 669: loss 0.209150\n",
      "batch 670: loss 0.329209\n",
      "batch 671: loss 0.466393\n",
      "batch 672: loss 0.203446\n",
      "batch 673: loss 0.323878\n",
      "batch 674: loss 0.111491\n",
      "batch 675: loss 0.141765\n",
      "batch 676: loss 0.212903\n",
      "batch 677: loss 0.236166\n",
      "batch 678: loss 0.238100\n",
      "batch 679: loss 0.316516\n",
      "batch 680: loss 0.204399\n",
      "batch 681: loss 0.233931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 682: loss 0.345665\n",
      "batch 683: loss 0.118268\n",
      "batch 684: loss 0.120656\n",
      "batch 685: loss 0.074644\n",
      "batch 686: loss 0.234716\n",
      "batch 687: loss 0.334899\n",
      "batch 688: loss 0.130264\n",
      "batch 689: loss 0.260818\n",
      "batch 690: loss 0.178961\n",
      "batch 691: loss 0.129747\n",
      "batch 692: loss 0.271902\n",
      "batch 693: loss 0.130008\n",
      "batch 694: loss 0.321945\n",
      "batch 695: loss 0.109755\n",
      "batch 696: loss 0.208691\n",
      "batch 697: loss 0.401018\n",
      "batch 698: loss 0.208215\n",
      "batch 699: loss 0.146073\n",
      "batch 700: loss 0.275273\n",
      "batch 701: loss 0.236203\n",
      "batch 702: loss 0.179597\n",
      "batch 703: loss 0.137024\n",
      "batch 704: loss 0.174632\n",
      "batch 705: loss 0.194961\n",
      "batch 706: loss 0.212014\n",
      "batch 707: loss 0.183293\n",
      "batch 708: loss 0.371186\n",
      "batch 709: loss 0.205685\n",
      "batch 710: loss 0.077059\n",
      "batch 711: loss 0.276888\n",
      "batch 712: loss 0.198075\n",
      "batch 713: loss 0.210584\n",
      "batch 714: loss 0.107578\n",
      "batch 715: loss 0.397799\n",
      "batch 716: loss 0.078945\n",
      "batch 717: loss 0.415044\n",
      "batch 718: loss 0.478946\n",
      "batch 719: loss 0.123193\n",
      "batch 720: loss 0.127362\n",
      "batch 721: loss 0.120526\n",
      "batch 722: loss 0.249230\n",
      "batch 723: loss 0.315637\n",
      "batch 724: loss 0.181162\n",
      "batch 725: loss 0.181041\n",
      "batch 726: loss 0.188831\n",
      "batch 727: loss 0.235881\n",
      "batch 728: loss 0.136089\n",
      "batch 729: loss 0.321096\n",
      "batch 730: loss 0.184186\n",
      "batch 731: loss 0.320273\n",
      "batch 732: loss 0.138659\n",
      "batch 733: loss 0.119165\n",
      "batch 734: loss 0.138783\n",
      "batch 735: loss 0.164813\n",
      "batch 736: loss 0.076197\n",
      "batch 737: loss 0.172055\n",
      "batch 738: loss 0.111935\n",
      "batch 739: loss 0.171364\n",
      "batch 740: loss 0.076691\n",
      "batch 741: loss 0.351762\n",
      "batch 742: loss 0.115583\n",
      "batch 743: loss 0.154013\n",
      "batch 744: loss 0.272067\n",
      "batch 745: loss 0.189709\n",
      "batch 746: loss 0.167146\n",
      "batch 747: loss 0.159513\n",
      "batch 748: loss 0.155982\n",
      "batch 749: loss 0.177679\n",
      "batch 750: loss 0.167315\n",
      "batch 751: loss 0.479934\n",
      "batch 752: loss 0.121158\n",
      "batch 753: loss 0.279892\n",
      "batch 754: loss 0.092053\n",
      "batch 755: loss 0.180173\n",
      "batch 756: loss 0.132651\n",
      "batch 757: loss 0.193360\n",
      "batch 758: loss 0.186236\n",
      "batch 759: loss 0.243060\n",
      "batch 760: loss 0.198364\n",
      "batch 761: loss 0.226664\n",
      "batch 762: loss 0.185946\n",
      "batch 763: loss 0.465746\n",
      "batch 764: loss 0.089039\n",
      "batch 765: loss 0.302531\n",
      "batch 766: loss 0.235058\n",
      "batch 767: loss 0.316825\n",
      "batch 768: loss 0.224612\n",
      "batch 769: loss 0.097103\n",
      "batch 770: loss 0.131935\n",
      "batch 771: loss 0.122659\n",
      "batch 772: loss 0.225024\n",
      "batch 773: loss 0.157642\n",
      "batch 774: loss 0.281776\n",
      "batch 775: loss 0.463265\n",
      "batch 776: loss 0.389217\n",
      "batch 777: loss 0.123222\n",
      "batch 778: loss 0.205690\n",
      "batch 779: loss 0.155940\n",
      "batch 780: loss 0.101090\n",
      "batch 781: loss 0.086173\n",
      "batch 782: loss 0.084398\n",
      "batch 783: loss 0.127101\n",
      "batch 784: loss 0.211724\n",
      "batch 785: loss 0.151875\n",
      "batch 786: loss 0.192708\n",
      "batch 787: loss 0.066767\n",
      "batch 788: loss 0.257880\n",
      "batch 789: loss 0.202211\n",
      "batch 790: loss 0.553309\n",
      "batch 791: loss 0.196701\n",
      "batch 792: loss 0.191177\n",
      "batch 793: loss 0.099579\n",
      "batch 794: loss 0.324116\n",
      "batch 795: loss 0.111958\n",
      "batch 796: loss 0.222572\n",
      "batch 797: loss 0.318172\n",
      "batch 798: loss 0.460785\n",
      "batch 799: loss 0.135515\n",
      "batch 800: loss 0.169017\n",
      "batch 801: loss 0.101493\n",
      "batch 802: loss 0.305110\n",
      "batch 803: loss 0.206707\n",
      "batch 804: loss 0.165659\n",
      "batch 805: loss 0.171425\n",
      "batch 806: loss 0.141644\n",
      "batch 807: loss 0.253287\n",
      "batch 808: loss 0.220787\n",
      "batch 809: loss 0.146370\n",
      "batch 810: loss 0.290458\n",
      "batch 811: loss 0.357504\n",
      "batch 812: loss 0.213385\n",
      "batch 813: loss 0.086759\n",
      "batch 814: loss 0.073545\n",
      "batch 815: loss 0.219183\n",
      "batch 816: loss 0.202700\n",
      "batch 817: loss 0.177479\n",
      "batch 818: loss 0.087104\n",
      "batch 819: loss 0.093487\n",
      "batch 820: loss 0.295752\n",
      "batch 821: loss 0.175832\n",
      "batch 822: loss 0.180550\n",
      "batch 823: loss 0.119736\n",
      "batch 824: loss 0.175341\n",
      "batch 825: loss 0.331100\n",
      "batch 826: loss 0.139719\n",
      "batch 827: loss 0.196927\n",
      "batch 828: loss 0.293297\n",
      "batch 829: loss 0.223914\n",
      "batch 830: loss 0.251242\n",
      "batch 831: loss 0.204451\n",
      "batch 832: loss 0.036618\n",
      "batch 833: loss 0.351883\n",
      "batch 834: loss 0.225023\n",
      "batch 835: loss 0.125788\n",
      "batch 836: loss 0.198353\n",
      "batch 837: loss 0.168349\n",
      "batch 838: loss 0.237845\n",
      "batch 839: loss 0.176525\n",
      "batch 840: loss 0.209619\n",
      "batch 841: loss 0.132011\n",
      "batch 842: loss 0.274312\n",
      "batch 843: loss 0.282661\n",
      "batch 844: loss 0.296520\n",
      "batch 845: loss 0.244539\n",
      "batch 846: loss 0.199066\n",
      "batch 847: loss 0.233124\n",
      "batch 848: loss 0.189824\n",
      "batch 849: loss 0.224957\n",
      "batch 850: loss 0.100501\n",
      "batch 851: loss 0.440801\n",
      "batch 852: loss 0.203898\n",
      "batch 853: loss 0.247228\n",
      "batch 854: loss 0.118299\n",
      "batch 855: loss 0.298045\n",
      "batch 856: loss 0.077420\n",
      "batch 857: loss 0.283257\n",
      "batch 858: loss 0.357999\n",
      "batch 859: loss 0.152321\n",
      "batch 860: loss 0.133843\n",
      "batch 861: loss 0.080089\n",
      "batch 862: loss 0.359914\n",
      "batch 863: loss 0.219909\n",
      "batch 864: loss 0.091676\n",
      "batch 865: loss 0.089371\n",
      "batch 866: loss 0.341203\n",
      "batch 867: loss 0.156567\n",
      "batch 868: loss 0.198837\n",
      "batch 869: loss 0.187263\n",
      "batch 870: loss 0.257203\n",
      "batch 871: loss 0.104366\n",
      "batch 872: loss 0.206729\n",
      "batch 873: loss 0.346321\n",
      "batch 874: loss 0.196606\n",
      "batch 875: loss 0.088027\n",
      "batch 876: loss 0.115334\n",
      "batch 877: loss 0.194305\n",
      "batch 878: loss 0.435878\n",
      "batch 879: loss 0.099300\n",
      "batch 880: loss 0.080439\n",
      "batch 881: loss 0.144602\n",
      "batch 882: loss 0.154387\n",
      "batch 883: loss 0.283538\n",
      "batch 884: loss 0.220278\n",
      "batch 885: loss 0.119167\n",
      "batch 886: loss 0.155994\n",
      "batch 887: loss 0.258872\n",
      "batch 888: loss 0.125439\n",
      "batch 889: loss 0.321857\n",
      "batch 890: loss 0.199910\n",
      "batch 891: loss 0.242069\n",
      "batch 892: loss 0.083538\n",
      "batch 893: loss 0.264083\n",
      "batch 894: loss 0.280649\n",
      "batch 895: loss 0.168616\n",
      "batch 896: loss 0.257966\n",
      "batch 897: loss 0.114152\n",
      "batch 898: loss 0.214424\n",
      "batch 899: loss 0.127552\n",
      "batch 900: loss 0.192423\n",
      "batch 901: loss 0.138387\n",
      "batch 902: loss 0.200016\n",
      "batch 903: loss 0.140380\n",
      "batch 904: loss 0.095381\n",
      "batch 905: loss 0.355592\n",
      "batch 906: loss 0.316185\n",
      "batch 907: loss 0.167995\n",
      "batch 908: loss 0.209547\n",
      "batch 909: loss 0.166017\n",
      "batch 910: loss 0.195879\n",
      "batch 911: loss 0.155291\n",
      "batch 912: loss 0.173153\n",
      "batch 913: loss 0.135156\n",
      "batch 914: loss 0.187282\n",
      "batch 915: loss 0.163307\n",
      "batch 916: loss 0.254026\n",
      "batch 917: loss 0.171502\n",
      "batch 918: loss 0.077835\n",
      "batch 919: loss 0.277530\n",
      "batch 920: loss 0.123656\n",
      "batch 921: loss 0.367310\n",
      "batch 922: loss 0.103399\n",
      "batch 923: loss 0.280941\n",
      "batch 924: loss 0.088334\n",
      "batch 925: loss 0.175653\n",
      "batch 926: loss 0.247964\n",
      "batch 927: loss 0.110442\n",
      "batch 928: loss 0.176235\n",
      "batch 929: loss 0.128043\n",
      "batch 930: loss 0.244758\n",
      "batch 931: loss 0.134574\n",
      "batch 932: loss 0.189440\n",
      "batch 933: loss 0.261292\n",
      "batch 934: loss 0.235591\n",
      "batch 935: loss 0.103357\n",
      "batch 936: loss 0.215295\n",
      "batch 937: loss 0.460149\n",
      "batch 938: loss 0.178614\n",
      "batch 939: loss 0.069145\n",
      "batch 940: loss 0.109006\n",
      "batch 941: loss 0.197030\n",
      "batch 942: loss 0.221453\n",
      "batch 943: loss 0.284126\n",
      "batch 944: loss 0.133105\n",
      "batch 945: loss 0.297606\n",
      "batch 946: loss 0.283104\n",
      "batch 947: loss 0.124382\n",
      "batch 948: loss 0.169467\n",
      "batch 949: loss 0.140543\n",
      "batch 950: loss 0.096578\n",
      "batch 951: loss 0.241099\n",
      "batch 952: loss 0.191811\n",
      "batch 953: loss 0.273079\n",
      "batch 954: loss 0.096240\n",
      "batch 955: loss 0.104020\n",
      "batch 956: loss 0.092738\n",
      "batch 957: loss 0.145159\n",
      "batch 958: loss 0.221940\n",
      "batch 959: loss 0.306018\n",
      "batch 960: loss 0.230191\n",
      "batch 961: loss 0.127036\n",
      "batch 962: loss 0.192988\n",
      "batch 963: loss 0.157085\n",
      "batch 964: loss 0.141754\n",
      "batch 965: loss 0.210224\n",
      "batch 966: loss 0.118622\n",
      "batch 967: loss 0.095613\n",
      "batch 968: loss 0.312391\n",
      "batch 969: loss 0.225738\n",
      "batch 970: loss 0.308646\n",
      "batch 971: loss 0.228535\n",
      "batch 972: loss 0.195669\n",
      "batch 973: loss 0.123730\n",
      "batch 974: loss 0.162467\n",
      "batch 975: loss 0.112688\n",
      "batch 976: loss 0.176276\n",
      "batch 977: loss 0.124153\n",
      "batch 978: loss 0.096358\n",
      "batch 979: loss 0.111869\n",
      "batch 980: loss 0.294990\n",
      "batch 981: loss 0.187825\n",
      "batch 982: loss 0.176222\n",
      "batch 983: loss 0.226657\n",
      "batch 984: loss 0.174547\n",
      "batch 985: loss 0.430319\n",
      "batch 986: loss 0.219060\n",
      "batch 987: loss 0.093995\n",
      "batch 988: loss 0.153708\n",
      "batch 989: loss 0.370693\n",
      "batch 990: loss 0.241438\n",
      "batch 991: loss 0.092574\n",
      "batch 992: loss 0.110126\n",
      "batch 993: loss 0.218115\n",
      "batch 994: loss 0.376274\n",
      "batch 995: loss 0.288542\n",
      "batch 996: loss 0.169034\n",
      "batch 997: loss 0.248657\n",
      "batch 998: loss 0.199838\n",
      "batch 999: loss 0.111574\n"
     ]
    }
   ],
   "source": [
    "summary_writer = tf.summary.create_file_writer('./tensorboard')     # 实例化记录器\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "        with summary_writer.as_default():                           # 指定记录器\n",
    "            tf.summary.scalar(\"loss\", loss, step=batch_index)       # 将当前损失函数的值写入记录器\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the direct of the tensorboard.py and use the commanda: tensorboard PATH --logdir logDIRECTORY. Open the directory rather than the file!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
